# GPU Tools

This page presents common tools and utilities for GPU computing.

# nvidia-smi

This is the NVIDIA Systems Management Interface. This utility can be used to monitor GPU usage and GPU memory usage. It is a comprehensive tool with many options.

```
$ nvidia-smi
       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-PCIE...  On   | 00000000:14:00.0 Off |                    0 |
| N/A   37C    P0    69W / 250W |   1475MiB / 32510MiB |     53%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  Tesla V100-PCIE...  On   | 00000000:39:00.0 Off |                    0 |
| N/A   33C    P0    24W / 250W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  Tesla V100-PCIE...  On   | 00000000:88:00.0 Off |                    0 |
| N/A   32C    P0    26W / 250W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  Tesla V100-PCIE...  On   | 00000000:B1:00.0 Off |                    0 |
| N/A   32C    P0    26W / 250W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A   2183874      C   ...envs/torch-env/bin/python     1471MiB |
+-----------------------------------------------------------------------------+
```

To see all of the available options, view the help:

```$ nvidia-smi --help```

Here is an an example that produces CSV output of various metrics:

```
$ nvidia-smi --query-gpu=timestamp,utilization.memory,memory.total,memory.free,memory.used --format=csv -l 5
```

The command above takes a reading every 5 seconds.

# gpustat

An alternative to `nvidia-smi` is `gpustat`. It also pulls its data from the NVIDIA Management Library (NVML).

```
$ gpustat
adroit-h11g1             Tue Oct 12 10:46:22 2021  470.57.02
[0] Tesla V100-PCIE-32GB | 38째C,  53 % |  1475 / 32510 MB | aturing(1471M)
[1] Tesla V100-PCIE-32GB | 33째C,   0 % |     0 / 32510 MB |
[2] Tesla V100-PCIE-32GB | 32째C,   0 % |     0 / 32510 MB |
[3] Tesla V100-PCIE-32GB | 32째C,   0 % |     0 / 32510 MB |
```

`gpustat` includes the NetID of the user in its output.

# Nsight Systems (nsys) for Profiling

The `nsys` command can be used to generate a timeline of the execution of your code. `nsys-ui` provides a GUI to examine the profiling data generated by `nsys`. See the NVIDIA Nsight Systems [getting started guide](https://docs.nvidia.com/nsight-systems/) and notes on [Summit](https://docs.olcf.ornl.gov/systems/summit_user_guide.html#profiling-gpu-code-with-nvidia-developer-tools).

To see the help menu:

```
$ module load cudatoolkit/11.7
$ nsys --help
$ nsys --help profile
```

IMPORTANT: Do not run profiling jobs in your `/home` directory because large files are often written during these jobs which can exceed your quota. Instead launch jobs from `/scratch/gpfs/<YourNetID>` where you have lots of space. Here's an example:

```
$ ssh <YourNetID>@della-gpu.princeton.edu
$ cd /scratch/gpfs/<YourNetID>
$ mkdir myjob && cd myjob
# prepare Slurm script
$ sbatch job.slurm
```

Below is an example Slurm script:

```
#!/bin/bash
#SBATCH --job-name=profile       # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=1        # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --mem=4G                 # total memory per node
#SBATCH --gres=gpu:1             # number of gpus per node
#SBATCH --time=00:10:00          # total run time limit (HH:MM:SS)

module purge
module load anaconda3/2022.5
conda activate myenv

nsys profile --trace=cuda,nvtx,osrt -o myprofile_${SLURM_JOBID} python myscript.py
```

For an MPI code you should use:

```
srun --wait=0 nsys profile --trace=cuda,nvtx,osrt,mpi -o myprofile_${SLURM_JOBID} ./my_mpi_exe
```

Note that `nsys-ui` does not exist for Traverse. You can download the `.qdrep` file to your local machine to use `nsys-ui` to view the data or do `ssh -X tigressdata.princeton.edu` and use `nsys-ui` on that machine. The latter approach, on Della, would look like this:

```
# in a new terminal
$ ssh -X <YourNetID>@tigressdata.princeton.edu
$ cd /della/scratch/gpfs/<YourNetID>/myjob
$ nsys-ui myprofile-*.qdrep
```

Run this command to see the summary statistics: `nsys stats myprofile_*.qdrep`.

# Nsight Compute (ncu) for GPU Kernel Profiling

The `ncu` command is used for detailed profiling of GPU kernels. See the NVIDIA [documentation](https://docs.nvidia.com/nsight-compute/). On some clusters you will need to load a module to make the command available:

```
$ ncu --help
bash: ncu: command not found
$ module load cudatoolkit/11.7
$ ncu --help
```

The idea is to use `ncu` for the profiling and `ncu-ui` for examining the data in a GUI.

Below is a sample slurm script:

```
#!/bin/bash
#SBATCH --job-name=profile       # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --ntasks=1               # total number of tasks across all nodes
#SBATCH --cpus-per-task=1        # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --mem=4G                 # total memory per node
#SBATCH --gres=gpu:1             # number of gpus per node
#SBATCH --time=00:10:00          # total run time limit (HH:MM:SS)

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK

module purge
module load anaconda3/2022.5
module load cudatoolkit/11.7
conda activate dark-env

ncu -o my_report_${SLURM_JOBID} python myscript.py
```

After the job finishes, one can use `ncu-ui` to view the results:

```
$ ssh -X <YourNetID>@della-vis2.princeton.edu
$ module load cudatoolkit/11.7
$ ncu-ui my_report_*.ncu-rep
```

The `ncu` profiler slows down the execution time of the code. Note that `ncu-ui` is not available for Traverse. You will need to examine the report file on a different machine like Tigressdata or your laptop.

# nvprof

This is the older NVIDIA profiler. It has been replaced by the combination of nsys and nv-nsight-cu-cli.

# line_prof for Profiling

The [line_prof](https://researchcomputing.princeton.edu/python-profiling) tool provides profiling info for each line of a function. It is easy to use and it can be used for Python codes that run on CPUs and/or GPUs.

# nvcc

This is the NVIDIA CUDA compiler. It is based on LLVM. To compile a simple code:

```
$ module load cudatoolkit/11.6
$ nvcc -o hello_world hello_world.cu
```

# stats.rc.princeton.edu

Follow [this procedure](https://researchcomputing.princeton.edu/support/knowledge-base/job-stats) to view detailed metrics for your Slurm jobs. This includes GPU utilization and memory as a function of time.

# GPU Computing

See [this page](https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing) for an overview of the hardware at Princton as well as useful commands like `gpudash` and `shownodes`.

# ARM DDT

The general directions for using the DDT debugger are [here](https://researchcomputing.princeton.edu/faq/debugging-with-ddt-on-the). The getting started guide is [here](https://developer.arm.com/tools-and-software/server-and-hpc/debug-and-profile/arm-forge/arm-ddt).

```
$ ssh -X <NetID>@adroit.princeton.edu
$ git clone https://github.com/PrincetonUniversity/hpc_beginning_workshop
$ cd hpc_beginning_workshop/RC_example_jobs/simple_gpu_kernel
$ salloc -N 1 -n 1 -t 10:00 --gres=gpu:1 --x11
$ module load cudatoolkit/10.1
$ nvcc -g -G hello_world_gpu.cu
$ module load ddt/20.0.1
$ export ALLINEA_FORCE_CUDA_VERSION=10.1
$ ddt
# check cuda, uncheck "submit to queue", and click on "Run"
```

The `-g` debugging flag is for CPU code while the `-G` flag is for GPU code. `-G` turns off compiler optimizations. Note that as of February 2020 CUDA Toolkit 10.2 is not supported.

If the graphics are not displaying fast enough then consider using [TurboVNC](https://researchcomputing.princeton.edu/faq/how-do-i-use-vnc-on-tigre).
